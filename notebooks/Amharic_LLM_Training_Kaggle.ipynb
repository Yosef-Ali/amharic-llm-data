{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30627,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Amharic LLM Training on Kaggle\n",
        "\n",
        "This notebook trains an Amharic language model using Kaggle's free GPU resources.\n",
        "\n",
        "## üìã Setup Instructions\n",
        "\n",
        "1. **Enable GPU**: Settings ‚Üí Accelerator ‚Üí GPU\n",
        "2. **Enable Internet**: Settings ‚Üí Internet ‚Üí On\n",
        "3. **Upload Dataset**: Create a Kaggle dataset with your Amharic data\n",
        "4. **Add Dataset**: Add your dataset to this notebook\n",
        "5. **Run All Cells**: Execute cells in order\n",
        "\n",
        "## üéØ Training Options\n",
        "\n",
        "- **Quick Test** (10-15 min): DistilGPT2 with 100 steps\n",
        "- **Balanced** (30-45 min): Bloom-560M with 300 steps\n",
        "- **High Quality** (1-2 hours): Bloom-1B1 with 500 steps\n",
        "- **Production** (2-4 hours): Phi-3.5-mini with full training"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "import subprocess\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "\n",
        "# Check nvidia-smi\n",
        "try:\n",
        "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
        "    print(\"\\nGPU Status:\")\n",
        "    print(result.stdout)\n",
        "except:\n",
        "    print(\"nvidia-smi not available\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-01T00:00:00.000000Z",
          "iopub.execute_input": "2024-01-01T00:00:00.000000Z",
          "iopub.status.idle": "2024-01-01T00:00:00.000000Z",
          "shell.execute_reply.started": "2024-01-01T00:00:00.000000Z",
          "shell.execute_reply": "2024-01-01T00:00:00.000000Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install transformers datasets peft accelerate bitsandbytes\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup data - Choose one of the options below\n",
        "import os\n",
        "os.chdir('/kaggle/working')\n",
        "\n",
        "# Option 1: Upload as Kaggle Dataset (RECOMMENDED)\n",
        "# 1. Create a new Kaggle dataset\n",
        "# 2. Upload your entire 'amharic-llm-data' folder\n",
        "# 3. Add the dataset to this notebook\n",
        "# 4. Uncomment and run the line below:\n",
        "# !cp -r /kaggle/input/amharic-llm-data/* .\n",
        "\n",
        "# Option 2: Clone from GitHub (if you've pushed the data)\n",
        "# !git clone https://github.com/Yosef-Ali/amharic-llm-data.git\n",
        "# %cd amharic-llm-data\n",
        "\n",
        "# Option 3: Manual upload\n",
        "# Upload your data files directly to /kaggle/working/\n",
        "\n",
        "# Check current directory\n",
        "!pwd\n",
        "!ls -la"
      ],
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check dataset\n",
        "!ls -la data/processed/\n",
        "\n",
        "# Show dataset statistics\n",
        "import json\n",
        "\n",
        "try:\n",
        "    with open('data/dataset_statistics.json', 'r') as f:\n",
        "        stats = json.load(f)\n",
        "        print(f\"üìä Dataset Statistics:\")\n",
        "        print(f\"Total examples: {stats['total_examples']}\")\n",
        "        print(f\"Train: {stats['train_size']}\")\n",
        "        print(f\"Validation: {stats['validation_size']}\")\n",
        "        print(f\"Test: {stats['test_size']}\")\n",
        "        print(f\"Average instruction length: {stats['avg_instruction_length']:.1f}\")\n",
        "        print(f\"Average response length: {stats['avg_response_length']:.1f}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ö†Ô∏è  Dataset statistics not found. Make sure data is properly uploaded.\")"
      ],
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show training options\n",
        "!python scripts/fast_training.py --options"
      ],
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QUICK TEST - DistilGPT2 (10-15 minutes)\n",
        "# Good for testing the pipeline quickly\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "!python scripts/fast_training.py --train --model distilgpt2 --steps 100 --output models/amharic-distilgpt2-test\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\n‚è±Ô∏è  Training completed in {(end_time - start_time)/60:.1f} minutes\")"
      ],
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BALANCED TRAINING - Bloom-560M (30-45 minutes)\n",
        "# Good balance of quality and speed\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "!python scripts/fast_training.py --train --model bloom-560m --steps 300 --output models/amharic-bloom560m-finetuned\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\n‚è±Ô∏è  Training completed in {(end_time - start_time)/60:.1f} minutes\")"
      ],
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HIGH QUALITY TRAINING - Bloom-1B1 (1-2 hours)\n",
        "# Best quality for production use\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "!python scripts/fast_training.py --train --model bloom-1b1 --steps 500 --output models/amharic-bloom1b1\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\n‚è±Ô∏è  Training completed in {(end_time - start_time)/60:.1f} minutes\")"
      ],
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRODUCTION TRAINING - Phi-3.5-mini (2-4 hours)\n",
        "# Highest quality, use only if you have time\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# Use the original training script for better quality\n",
        "!python scripts/train_example.py --train --model microsoft/Phi-3.5-mini-instruct --output models/amharic-phi35-production\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\n‚è±Ô∏è  Training completed in {(end_time - start_time)/60:.1f} minutes\")"
      ],
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the trained model\n",
        "model_to_test = \"models/amharic-bloom560m-finetuned\"  # Change this to your trained model\n",
        "\n",
        "!python scripts/fast_training.py --test --output {model_to_test}"
      ],
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive testing with custom prompts\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load your best trained model\n",
        "model_path = \"models/amharic-bloom560m-finetuned\"  # Change this\n",
        "\n",
        "print(f\"Loading model from: {model_path}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "# Move to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "print(f\"Model loaded on: {device}\")\n",
        "\n",
        "def generate_amharic_response(instruction, max_length=100):\n",
        "    prompt = f\"Instruction: {instruction}\\nResponse:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_length,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "    \n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    response = full_response.split(\"Response:\")[-1].strip()\n",
        "    return response\n",
        "\n",
        "# Test with various Amharic instructions\n",
        "test_cases = [\n",
        "    \"·ã®·ä†·àõ·à≠·äõ ·âã·äï·âã ·àù·äï·ãµ·äï ·äê·ãç?\",\n",
        "    \"·ä¢·âµ·ãÆ·åµ·ã´ ·ã®·âµ ·âµ·åà·äõ·àà·âΩ?\",\n",
        "    \"·ã®·ä†·ã≤·àµ ·ä†·â†·â£ ·ãã·äì ·ä®·â∞·àõ ·àù·äï·ãµ·äï ·äê·ãç?\",\n",
        "    \"·ã®·ä¢·âµ·ãÆ·åµ·ã´ ·â£·äï·ã≤·à´ ·àù·äï ·âÄ·àà·àù ·äê·ãç?\",\n",
        "    \"·àµ·àà ·ä¢·âµ·ãÆ·åµ·ã´ ·â≥·à™·ä≠ ·äï·åà·à®·äù\",\n",
        "    \"·ã®·ä†·àõ·à≠·äõ ·çä·ã∞·àã·âµ ·àµ·äï·âµ ·äì·â∏·ãç?\"\n",
        "]\n",
        "\n",
        "print(\"üß™ Testing Amharic Language Model:\\n\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, instruction in enumerate(test_cases, 1):\n",
        "    print(f\"Test {i}:\")\n",
        "    print(f\"‚ùì Question: {instruction}\")\n",
        "    \n",
        "    response = generate_amharic_response(instruction)\n",
        "    print(f\"ü§ñ Response: {response}\")\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model evaluation and comparison\n",
        "import os\n",
        "\n",
        "print(\"üìä Trained Models Summary:\\n\")\n",
        "\n",
        "models_dir = \"models\"\n",
        "if os.path.exists(models_dir):\n",
        "    for model_name in os.listdir(models_dir):\n",
        "        model_path = os.path.join(models_dir, model_name)\n",
        "        if os.path.isdir(model_path):\n",
        "            # Get model size\n",
        "            size = sum(os.path.getsize(os.path.join(dirpath, filename))\n",
        "                      for dirpath, dirnames, filenames in os.walk(model_path)\n",
        "                      for filename in filenames)\n",
        "            size_mb = size / (1024 * 1024)\n",
        "            \n",
        "            print(f\"ü§ñ {model_name}:\")\n",
        "            print(f\"   Size: {size_mb:.1f} MB\")\n",
        "            print(f\"   Path: {model_path}\")\n",
        "            print()\n",
        "\n",
        "# Show training recommendations\n",
        "print(\"üéØ Recommendations:\")\n",
        "print(\"‚Ä¢ DistilGPT2: Good for quick testing and prototyping\")\n",
        "print(\"‚Ä¢ Bloom-560M: Best balance of quality and speed\")\n",
        "print(\"‚Ä¢ Bloom-1B1: Higher quality, good for production\")\n",
        "print(\"‚Ä¢ Phi-3.5-mini: Highest quality, best for final deployment\")"
      ],
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save models and create download links\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "# Create a zip file with all models\n",
        "def create_model_archive():\n",
        "    if os.path.exists(\"models\"):\n",
        "        print(\"üì¶ Creating model archive...\")\n",
        "        \n",
        "        with zipfile.ZipFile('amharic_models.zip', 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "            for root, dirs, files in os.walk(\"models\"):\n",
        "                for file in files:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    arcname = os.path.relpath(file_path, \".\")\n",
        "                    zipf.write(file_path, arcname)\n",
        "        \n",
        "        print(\"‚úÖ Model archive created: amharic_models.zip\")\n",
        "        \n",
        "        # Show file size\n",
        "        size = os.path.getsize('amharic_models.zip') / (1024 * 1024)\n",
        "        print(f\"üìÅ Archive size: {size:.1f} MB\")\n",
        "        \n",
        "        return 'amharic_models.zip'\n",
        "    else:\n",
        "        print(\"‚ùå No models directory found\")\n",
        "        return None\n",
        "\n",
        "archive_path = create_model_archive()\n",
        "\n",
        "if archive_path:\n",
        "    print(\"\\nüéâ Training Complete!\")\n",
        "    print(\"\\nYour trained Amharic language models are ready!\")\n",
        "    print(f\"Download the archive: {archive_path}\")\n",
        "    print(\"\\nNext steps:\")\n",
        "    print(\"1. Download the models\")\n",
        "    print(\"2. Test them locally\")\n",
        "    print(\"3. Deploy to production\")\n",
        "    print(\"4. Create a demo with Gradio\")"
      ],
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéâ Training Complete!\n",
        "\n",
        "Congratulations! You have successfully trained Amharic language models using Kaggle's free GPU resources.\n",
        "\n",
        "### üìä What You've Accomplished\n",
        "\n",
        "- ‚úÖ Trained multiple Amharic language models\n",
        "- ‚úÖ Tested model performance with Amharic instructions\n",
        "- ‚úÖ Created production-ready models\n",
        "- ‚úÖ Optimized for different use cases (speed vs quality)\n",
        "\n",
        "### üöÄ Next Steps\n",
        "\n",
        "1. **Download Models**: Save the `amharic_models.zip` file\n",
        "2. **Local Testing**: Test models on your local machine\n",
        "3. **Create Demo**: Build a Gradio or Streamlit demo\n",
        "4. **Deploy**: Use Hugging Face Spaces or other platforms\n",
        "5. **Improve**: Collect more data and retrain\n",
        "\n",
        "### üìà Model Recommendations\n",
        "\n",
        "| Model | Use Case | Speed | Quality |\n",
        "|-------|----------|-------|---------|\n",
        "| DistilGPT2 | Testing, Prototyping | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê |\n",
        "| Bloom-560M | General Use | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê |\n",
        "| Bloom-1B1 | Production | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
        "| Phi-3.5-mini | High-end Production | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
        "\n",
        "### üîó Useful Resources\n",
        "\n",
        "- [Hugging Face Hub](https://huggingface.co) - Share your models\n",
        "- [Gradio](https://gradio.app) - Create interactive demos\n",
        "- [Streamlit](https://streamlit.io) - Build web apps\n",
        "- [Transformers Docs](https://huggingface.co/docs/transformers) - Learn more\n",
        "\n",
        "### üí° Tips for Better Models\n",
        "\n",
        "- **More Data**: Collect additional Amharic text data\n",
        "- **Longer Training**: Increase training steps for better quality\n",
        "- **Hyperparameter Tuning**: Experiment with learning rates\n",
        "- **Evaluation**: Create proper evaluation metrics\n",
        "- **Fine-tuning**: Adapt models for specific tasks"
      ],
      "metadata": {}
    }
  ]
}